{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Supervised Learning Project: Big Mart Sales\n\nBigMart is a big supermarket chain, with stores all around the country. The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and predict the sales of each product at a particular outlet.\n\n* **Data Dictionary**\n\nWe have a train (8523) and test (5681) data set, the train data set has both input and output variable(s).\n\nTrain file:\nCSV containing the item outlet information with a sales value\n\nVariable Description\n* ItemIdentifier ---- Unique product ID\n* ItemWeight ---- Weight of product\n* ItemFatContent ---- Whether the product is low fat or not\n* ItemVisibility ---- The % of the total display area of all products in a store allocated to the particular product\n* ItemType ---- The category to which the product belongs\n* ItemMRP ---- Maximum Retail Price (list price) of the product\n* OutletIdentifier ---- Unique store ID\n* OutletEstablishmentYear ---- The year in which the store was established\n* OutletSize ---- The size of the store in terms of ground area covered\n* OutletLocationType ---- The type of city in which the store is located\n* OutletType ---- Whether the outlet is just a grocery store or some sort of supermarket\n* ItemOutletSales ---- sales of the product in particular store. This is the outcome variable to be predicted.\n\n\n\nTest file:\nCSV containing item outlet combinations for which sales need to be forecasted\n\n* Variable Description\n* ItemIdentifier ----- Unique product ID\n* ItemWeight ---- Weight of product\n* ItemFatContent ----- Whether the product is low fat or not\n* ItemVisibility ---- The % of the total display area of all products in a store allocated to the particular product\n* ItemType ---- The category to which the product belongs\n* ItemMRP ----- Maximum Retail Price (list price) of the product\n* OutletIdentifier ----- Unique store ID\n* OutletEstablishmentYear ----- The year in which store store was established\n* OutletSize ----- The size of the store in terms of ground area covered\n* OutletLocationType ---- The type of city in which the store is located\n* OutletType ---- whether the outlet is just a grocery store or some sort of supermarket\n"},{"metadata":{},"cell_type":"markdown","source":"This is a supervised machine learning problem with a target label as: Item_Outlet_Sales \n\nSince the aim is predict the sales for test dataset, this is a regression task."},{"metadata":{},"cell_type":"markdown","source":"Importing libraries necessary for this project."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# Libraries for manipulate the data.\nimport pandas as pd\nimport numpy as np\n\n# Libraries for data visualization.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Libraries for model building.\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nExploratory data analysis is an approach to analyzing data sets and extracting useful information from the data. The analysis starts from the descriptive exploration of the data such as number of missing records and values to a visual exploration in order to better represent the data in more intuitive formats. This technique often using statistical graphics and other data visualization methods."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Load dataset.\ndf_train = pd.read_csv('../input/big-mart-salescsv/Train_UWu5bXk.csv')\ndf_test = pd.read_csv('../input/big-mart-salescsv/Test_u94Q5KV.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check the shape of the data.\nprint('Training data: {}'.format(df_train.shape))\nprint('Test data: {}'.format(df_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check for null values on training data.\nprint(df_train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check for null values on test data.\nprint(df_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Generate descriptive statistics on training data.\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Generate descriptive statistics on test data.\ndf_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The graphs below show the univariate distribution data of the numeric variables.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\n\nfor column in df_train.describe().columns:\n    sns.displot(df_train[column].dropna(), kde=True, element='step')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Boxplot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in df_train.describe().columns:\n    sns.boxplot(x=df_train[column].dropna())\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relationship between variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in df_train.describe().columns:\n    sns.relplot(data=df_train.dropna(), x=column, y='Item_Outlet_Sales')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis of the 'Item_Type' categorical variable to see the distribution of the items sold on outlets**\n\nAmong the products sold, the 'Fruits and Vegetables' are the most sold items, while 'Seafood' are the least sold."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nax = sns.countplot(x=df_train['Item_Type'])\nplt.xticks(rotation=90)\nplt.show()\n\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()+20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distribution of the 'Outlet_Size' variable**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.countplot(x=df_train['Outlet_Size'])\nplt.show()\n\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distribution of the 'Outlet_Location_Type' variable**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.countplot(x=df_train['Outlet_Location_Type'])\nplt.show()\n\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distribution of the 'Outlet_Type' variable**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nax = sns.countplot(x=df_train['Outlet_Type'])\nplt.show()\n\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Impact of the 'Item_Fat_Content' on 'Item_Outlet_Sales'**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Resolve naming discrepancies on 'Item_Fat_Content' variable.\ndf_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].replace({'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'})\n\ndf_item_fat_pivot = df_train.pivot_table(index='Item_Fat_Content', values='Item_Outlet_Sales', aggfunc=np.median)\n\nax = df_item_fat_pivot.plot(kind='bar', color='blue', figsize=(12,8), alpha=0.6)\nplt.ylabel('Item_Outlet_Sales')\nplt.title('Impact of the Item_Fat_Content on Item_Outlet_Sales')\nplt.xticks(rotation=0)\nplt.show()\n\nfor p in ax.patches:\n    ax.annotate('{:.2f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()+30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Impact of the 'Outlet_Type' on 'Item_Outlet_Sales'**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_outlet_type_pivot = df_train.pivot_table(index='Outlet_Type', values='Item_Outlet_Sales', aggfunc=np.median)\n\nax = df_outlet_type_pivot.plot(kind='bar', color='maroon', figsize=(12,8), alpha=0.6)\nplt.ylabel('Outlet_Type')\nplt.title('Impact of the Outlet_Type on Item_Outlet_Sales')\nplt.xticks(rotation=0)\nplt.show()\n\nfor p in ax.patches:\n    ax.annotate('{:.2f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()+30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation Matrix**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax = sns.heatmap(df_train.corr(), annot=True, square=True, cmap='inferno')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Enginnering\n\nFeature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. This process has two main goals:\n\n- Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n- Improving the performance of machine learning models.\n"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Join training and test data to apply data mining techniques.\ndf_train_aux = df_train.copy()\ndf_test_aux = df_test.copy()\ndf_train_aux['Source_Data'] = 'Train'\ndf_test_aux['Source_Data'] = 'Test'\n\ndf_data = pd.concat([df_train_aux, df_test_aux], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Resolve naming discrepancies on 'Item_Fat_Content' variable.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data['Item_Fat_Content'] = df_data['Item_Fat_Content'].replace({'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'})\n\ndf_data['Item_Fat_Content'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Treat Missing Values (NaN)**\n\n- Item_Weight\n\n    Analysing the Boxplot graph of the Item_Weight variable, it's possible to assume that it has approximately an normal distribution    (Gaussian distribution). In this case, the missing values can be replaced by the median of the Item_Weight column."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data['Item_Weight'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Replace missing values on Item_Weight column.\ndf_data['Item_Weight'] = df_data['Item_Weight'].fillna(df_data['Item_Weight'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Outlet_Size\n    \n    The missing values of the Outlet_Size column will be replaced by the 'Medium', because it is the value most frequently on column."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data['Outlet_Size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Replace missing values on Outlet_Size column.\ndf_data['Outlet_Size'] = df_data['Outlet_Size'].fillna('Medium')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Item_Visibility\n\n    The Item_Visibility column has some items with value 0 (no visibility), but all the items needs to be visible to the customers. This means that those items was not available and were marked as 0. Therefore, it's need to treat this as missing values."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Amount of items marked with visility 0.\ndf_data[df_data['Item_Visibility'] == 0]['Item_Visibility'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the approach followed here, will be to replace those missing values with the median of the column. Since, the Boxplot graph of the Item_Visibility apresents some outliers. The median is less sensible to outliers than the mean."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data['Item_Visibility'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Replace values 0 on Item_Visibility column.\ndf_data.loc[df_data['Item_Visibility']<=0 , 'Item_Visibility'] = df_data['Item_Visibility'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Outlet_Establishment_Year"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data['Outlet_Establishment_Year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data['Outlet_Years'] = 2013 - df_data['Outlet_Establishment_Year']\ndf_data['Outlet_Years'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Item_Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data['Item_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Item_Identifier\n\nAnalysing the data, it's possible to note that those item types are divided in three main categories which are Food, Drink and Non-Consumable."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df_data['Item_Identifier'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's possible to note that the item names starts with either 'FD' (Food), 'DR' (Drink) or 'NC' (Non-Consumable)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get only the first two characters.\ndf_data['New_Item_Type'] = df_data['Item_Identifier'].apply(lambda x: x[0:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Rename the 'New_Item_Type' to more intuitive categories.\ndf_data['New_Item_Type'] = df_data['New_Item_Type'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drink'}) \n\ndf_data['New_Item_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mark non-consumables as separate category in 'Item_Fat_Content'."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.loc[df_data['New_Item_Type'] == 'Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'\n\ndf_data['Item_Fat_Content'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate the visibility average of each product."},{"metadata":{"trusted":true},"cell_type":"code","source":"item_visibility_avg = df_data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n\nitem_visibility_avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"function = lambda x: x['Item_Visibility']/item_visibility_avg['Item_Visibility'][item_visibility_avg.index == x['Item_Identifier']][0]\ndf_data['Item_Visibility_Avg'] = df_data.apply(function, axis=1).astype(float)\n\ndf_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transform categorical variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use One-hot encoding.\ndf_data = pd.get_dummies(df_data, prefix=['Item', 'Outlet', 'Outlet', 'Outlet', 'Outlet', 'Outlet'], columns=['Item_Fat_Content', \n                                         'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'New_Item_Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.iloc[:, :15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns that doesn't will be used on model training.\ndf_mdl = df_data.drop(columns=['Item_Identifier', 'Item_Type', 'Outlet_Establishment_Year'])\n\ndf_mdl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data in training and test sets.\ndf_mdl_train = df_mdl.loc[df_mdl['Source_Data'] == 'Train']\ndf_mdl_test = df_mdl.loc[df_mdl['Source_Data'] == 'Test']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mdl_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mdl_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove columns that doesn't will be used.\ndf_mdl_train = df_mdl_train.drop(columns=['Source_Data'])\ndf_mdl_test = df_mdl_test.drop(columns=['Item_Outlet_Sales', 'Source_Data'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mdl_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = df_mdl_train.drop(columns=['Item_Outlet_Sales']).to_numpy()\ny_train = df_mdl_train['Item_Outlet_Sales'].to_numpy()\n\nx_test = df_mdl_test.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize features.\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\n\nx_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\n\nRandom forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or average prediction (regression) of the individual trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = RandomForestRegressor(n_estimators=100, random_state=42)\nreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg.predict(x_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = pd.DataFrame(columns=['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales'])\ndf_result['Item_Identifier'] = df_test['Item_Identifier']\ndf_result['Outlet_Identifier'] = df_test['Outlet_Identifier']\ndf_result['Item_Outlet_Sales'] = y_pred\n\ndf_result.to_csv('result.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Considerations\n\nThe Random forest algorithm obtained a good score (coefficient of determination r2) of 0.9 at training data.\n\nBut, there are others methods that can be used in this dataset. Examples:\n\n- Support Vector Regressor (SVR)\n- Bagging Regression\n- Gradient Boosting \n- Artificial Neural Networks (ANN)\n- Among others."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}